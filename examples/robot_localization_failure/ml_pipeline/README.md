# ğŸ“˜ **AMCL Failure Detection â€” ML Pipeline Documentation**

This repository provides a complete, modular, reproducible **machine learning pipeline** for detecting **AMCL delocalization** events using handcrafted features from:

* **Scanâ€“Map Statistics**
* **Particle Cloud Statistics**
* **AMCL & Ground-truth pose alignment**

The pipeline:

1. **Processes ROS2 bags**
2. **Extracts 40+ handcrafted features**
3. **Builds train/eval datasets**
4. **Trains ML models**
5. **Evaluates any saved model**
6. **Stores all results cleanly inside `artifacts/`**
7. **Can run each step individually or automatically**

---

# ğŸ“‚ **Directory Structure**

```
ml_pipeline/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ build_dataset.py      # Process all bags â†’ create training & eval datasets
â”‚   â”œâ”€â”€ bag_processor.py      # Core logic for handling a single bag file
â”‚   â”œâ”€â”€ helpers.py            # Common utilities (yaw, timeseries unpacking)
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ common.py             # Shared ML utilities (scaling, loading model package)
â”‚   â”œâ”€â”€ train_model_rf.py     # Train RandomForest classifier + save artifacts
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate_model.py     # Evaluate ANY model directory (interactive or CLI)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ paths.py              # Normalized paths for datasets, models, evaluation
â”‚   â”œâ”€â”€ cleaning.py           # Deletes the entire artifacts directory
â”‚
â”œâ”€â”€ artifacts/                # Auto-created, contains datasets/models/eval outputs
â”‚
â”œâ”€â”€ run_all.py                # One-shot: dataset â†’ training â†’ evaluation
â”œâ”€â”€ clean.py                  # Shortcut to wipe artifacts/
â””â”€â”€ README.md                 # This file
```

---

# âš™ï¸ **Configuration File**

You must place your `config.yaml` at:

```
robot_localization_failure/config.yaml
```

Example:

```yaml
rosbag:
  training_paths:
    - toy_data/training_data_half/rec_20250923_135805_id_01
    - toy_data/training_data_half/rec_20250923_135805_id_02

  evaluation_paths:
    - toy_data/testing_data_half/rec_20250923_142005_id_01
    - toy_data/testing_data_half/rec_20250923_142005_id_02

  message_paths:
    - ros_msgs/sensor_msgs/msg/LaserScan.msg
    - ros_msgs/nav2_msgs/msg/Particle.msg
    - ros_msgs/nav2_msgs/msg/ParticleCloud.msg

localization:
  position_threshold: 0.4
  heading_threshold: 0.4
```

This file controls **which bags to process** and the **AMCL failure criteria**.

---

# ğŸ“¦ **Artifacts Directory**

Everything the pipeline produces is saved inside:

```
ml_pipeline/artifacts/
```

Structure:

```
artifacts/
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ train.parquet
â”‚   â””â”€â”€ eval.parquet
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest_baseline/         # every training run creates its own folder
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”‚   â””â”€â”€ feature_columns.json
â”‚   â”œâ”€â”€ rf_experiment_02/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ evaluation/
    â”œâ”€â”€ model_name/
    â”‚   â”œâ”€â”€ eval_results.parquet
    â”‚   â””â”€â”€ metrics.txt  (optional)
```

This ensures:

* every experiment is preserved
* model/eval artifacts never overwrite each other
* the repository root stays clean

---

# ğŸ§  **Overview of the Pipeline Components**

## ğŸ”¹ 1. `build_dataset.py`

Processes all bags and produces:

```
artifacts/datasets/train.parquet
artifacts/datasets/eval.parquet
```

Each file contains:

* 15 Scanâ€“Map features
* 14 Particle cloud statistics
* 7 AMCL pose quantities
* 7 GT pose quantities
* Position & heading errors
* `is_delocalized` label

A total of **40+ ML features**.

---

## ğŸ”¹ 2. `train_model_rf.py`

Trains a baseline **Random Forest** classifier:

* loads `train.parquet`
* removes leakage columns
* scales features using `StandardScaler`
* trains RandomForest classifier
* saves everything to:

```
artifacts/models/<model_name>/
    model.pkl
    scaler.pkl
    feature_columns.json
```

Each training run creates a **new model directory**.

---

## ğŸ”¹ 3. `common.py`

This file provides **shared ML utilities**:

âœ” `load_model_package()`
Loads a stored model directory:

* model.pkl
* scaler.pkl
* feature_columns.json

âœ” `apply_scaler()`
Applies a stored scaler or returns raw features if none exists.

âœ” `timestamped_model_dir()`
Creates a new folder inside `artifacts/models/` automatically.

This is used by all training scripts to maintain clean, isolated experiment folders.

---

## ğŸ”¹ 4. `evaluate_model.py`

A **model-agnostic evaluator**.

### ğŸŸ¢ Supports:

### **1ï¸âƒ£ Interactive model selection**

```
python3 -m ml_pipeline.evaluation.evaluate_model
```

Output:

```
Available models:
[0] random_forest_2025_11_20_14_33_51
[1] rf_experiment_bigtrees
Select a model index:
```

### **2ï¸âƒ£ Direct model selection**

```
python3 -m ml_pipeline.evaluation.evaluate_model --model_dir random_forest_2025_11_20_14_33_51
```

### It performs:

* loads `eval.parquet`

* loads the chosen model

* scales features

* predicts labels + probabilities

* prints:

  âœ” classification report
  âœ” confusion matrix
  âœ” precision, recall, F1
  âœ” **F0.5 score (important for early detection)**

* saves predictions to:

```
artifacts/models/<model_dir>/eval_results.parquet
```

---

# ğŸš€ **How to Run the Pipeline**

## **1ï¸âƒ£ Build the training & evaluation datasets**

Extracts all handcrafted features and builds 40+ column ML tables.

```
python3 -m ml_pipeline.dataset.build_dataset
```

This creates:

```
artifacts/datasets/train.parquet
artifacts/datasets/eval.parquet
```

---

## **2ï¸âƒ£ Train a model**

Trains a Random Forest and saves artifacts.

```
python3 -m ml_pipeline.training.train_model_rf
```

This creates:

```
artifacts/models/random_forest_<timestamp>/
```

---

## **3ï¸âƒ£ Evaluate a model**

### **Option A â€” interactive choose model**

```
python3 -m ml_pipeline.evaluation.evaluate_model
```

### **Option B â€” specify model name**

```
python3 -m ml_pipeline.evaluation.evaluate_model --model_dir random_forest_2025_11_20_14_33_51
```

Outputs include:

* precision, recall
* confusion matrix
* **F0.5 score**
* predictions saved to:

```
artifacts/models/<model_dir>/eval_results.parquet
```

---

# ğŸ§¹ **Cleaning Everything**

To wipe all datasets, models, and evaluation outputs:

```
python3 -m ml_pipeline.clean
```

This deletes the entire `artifacts/` directory.

---

# ğŸ” **Run the Entire Pipeline Automatically**

```
python3 -m ml_pipeline.run_all
```

Runs:

```
build_dataset â†’ train_model â†’ evaluate_model
```

Everything is written into `artifacts/`.

---

# ğŸ§¬ **What the Pipeline Actually Computes**

### âœ” 1. Scanâ€“Map Statistics

Geometric scan-map alignment â†’ 15 features
(e.g., point_distance, line_angle, ray_qualityâ€¦)

### âœ” 2. Particle Cloud Statistics

Distribution shape â†’ 14 features
(center of gravity spread, cluster varianceâ€¦)

### âœ” 3. Pose Alignment

Time-synced AMCL & ground truth
â†’ computed yaw, errors, labels

### âœ” 4. Final ML Table

A clean, flat dataset suitable for:

* Random Forest
* XGBoost
* LightGBM
* Neural networks
* Optuna hyperparameter tuning
* Scikit-learn pipelines
* Neural networks

---

# ğŸ¯ **Next Steps (Planned Modules)**

You will soon add:

* `train_model_xgb.py` â€” XGBoost
* `train_model_lightgbm.py` â€” LightGBM
* `train_model_nn.py` â€” small MLP
* `evaluate_model.py` (extended) â€” ROC curves, PR curves
* `compare_models.py` â€” automatically compare all models
* `plot_feature_importance.py` â€” SHAP + RF/XGB importance

These will plug into the same `common.py` utilities.

---

# ğŸ‰ **Conclusion**

This pipeline is:

* modular
* reproducible
* extensible
* model-agnostic
* easy to run
* easy to clean
